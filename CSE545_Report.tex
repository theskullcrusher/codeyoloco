%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Template: Project Titlepage Modified (v 0.1) by rcx
%
% Original Source: http://www.howtotex.com
% Date: February 2014
% 
% This is a title page template which be used for articles & reports.
% 
% This is the modified version of the original Latex template from
% aforementioned website.
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{report}
\usepackage[a4paper]{geometry}
\usepackage[myheadings]{fullpage}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{graphicx, wrapfig, subcaption, setspace, booktabs}
\usepackage[T1]{fontenc}
\usepackage[font=small, labelfont=bf]{caption}
\usepackage{fourier}
\usepackage[protrusion=true, expansion=true]{microtype}
\usepackage[english]{babel}
\usepackage{sectsty}
\usepackage{url, lipsum}
\usepackage{hyperref}

\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\onehalfspacing
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}

%-------------------------------------------------------------------------------
% HEADER & FOOTER
%-------------------------------------------------------------------------------
\pagestyle{fancy}
\fancyhf{}
\setlength\headheight{15pt}
\fancyhead[L]{Code Yo Loco}
\fancyhead[R]{Arizona State University}
\fancyfoot[R]{Page \thepage\ of \pageref{LastPage}}
%-------------------------------------------------------------------------------
% TITLE PAGE
%-------------------------------------------------------------------------------

\begin{document}

\title{ \normalsize \textsc{Team Code Yo Loco}
		\\ [2.0cm]
		\HRule{0.5pt} \\
		\LARGE \textbf{\uppercase{An Approach to CTF Automation \\ PCTF Report}}
		\HRule{2pt} \\ [0.5cm]
		\normalsize \today \vspace*{5\baselineskip}}

\date{}

\author{Laurence Chang, James Hutchins, Deepak Krishnan, Mohit Mulchandani
		\\ Vishnu Radja, Suraj Shah, Josh Smith \\ \\ 
		Arizona State University \\
		CSE 545 }

\maketitle
\tableofcontents
\newpage

%-------------------------------------------------------------------------------
% Section title formatting
\sectionfont{\scshape}
%-------------------------------------------------------------------------------

%-------------------------------------------------------------------------------
% Abstract
%-------------------------------------------------------------------------------
\addcontentsline{toc}{section}{Abstract}
\section*{Abstract}
Our goal as team Code Yo Loco was to design an automated Python tool suite that would assist the team in both spectrums of attack and defense during the CSE 545 CTF. Our attack plan included identifying and exploiting at least one buffer overflow vulnerability, injecting shellcode which installs a malicious backdoor "flag monitor" on the victim machine, and stealing the flags directly from the file system. In addition, we used the example servers to develop additional tools that automated the collection of the flags. The correct flags will be part of the body section of a constructed HTTP packet sent to our host, in which another automated script will directly submit the flags to the master server. 

%-------------------------------------------------------------------------------
% Introduction
%-------------------------------------------------------------------------------
\addcontentsline{toc}{section}{Introduction}
\section*{Introduction}
A real time CTF requires the automation of working tools for the exploitation of services, defensive actions, and collection and submission of flags. This realization led to our efforts of attempting to minimize the amount of manual work required by the team members themselves, while ensuring most of the work load was automated by the various tools that were constructed. \\

\noindent The tools developed not only served as standalone methods of automating needed functions such as server login, flag submissions, and execution of exploits, but also served as modules that we could further build on top of to develop other tools. Due to the unknown nature of what the actual game-server will be like at the time of competition, we design our system with a couple of assumptions using the test-server as a base and foundation to work with. \\

\noindent Exploring the example CTF server, we found several key components that influenced the design of our project. First, the team's server ran three services, \texttt{sample.c}, \texttt{sample.py}, and \texttt{sample.web}, that we know are monitored by a "master" game server, running the Scriptbot and Gamebot, that check to see if the application is available and running or not. Bringing a service offline would result in the loss of points so this aspect of the game was something we had to consider in order to keep the availability of our services up. Another aspect of the CTF server we took into account was the interval of the ticks, or three minute rounds, that the server ran on. This observation was very important as this implied several consequences: \\

\begin{enumerate}
	\item The value of the flags in respect to their services change after every tick.
	\item Resubmission of the new flags are required.
	\item Opponent services may be patched, possibly increases the security of the application.
	
\end{enumerate}
Therefore our approach in regards to the list above involves a focus on exploiting the vulnerabilities as soon as possible, and then through the initial exploitation, we are able to deploy a long term solution to automate the collection and submission of the victim's flags. \\

\noindent In this report, we will cover our overall design including details of how each tool in our framework operates and the services they provide a CTF team. We follow up with explanations of how we developed our framework and reporting the justifications behind why these tools were built and the goal of what these tools are to achieve. We also justify the assumptions taken into account when developing the framework. We transition into the results we have achieved while also documenting some of the problems we had encountered or foresaw regarding our tools in relation to the actual competition. Finally we discuss the impact of our developments, tools, and observations in our conclusion.

%-------------------------------------------------------------------------------
% Design
%-------------------------------------------------------------------------------
\addcontentsline{toc}{section}{Design}
\section*{Design and Implementation}
Our design considers the following assumptions we made about the game server:

\begin{enumerate}
	\item There will be at \textbf{least} one buffer overflow that allows us to inject the shell code that is equivalent in accomplishing the following two functions:
	\begin{itemize}
		\item \texttt{wget <serverURI>} - This allows us to install the malicious backdoor on the victim machine
		\item \texttt{./backdoor.sh} - This allows us to immediately begin running the script after installation
	\end{itemize}
	\item We assume flags are being stored, read, and write in the directory: \\\\ \texttt{/opt/ctf/<some\_game\_service>/rw/} \\\\
	This assumption is made with the statement that all game servers will be similar with each other, including our own. 
	\item The existing services on the example server will also be present on the live server.\\ 
	The current services on the example server can be automated through a tool we construct. Furthermore, if the new services added on the live CTF server are similar to the existing services, in terms of vulnerability and input, our tools should be easily configurable to also exploit the new services.
\end{enumerate}

\noindent We first define our offensive approach. Using the iCTF library provided, we built on top of the existing functions to create custom scripts that automated many tasks that gave us an advantage such as auto exploiting services and auto submitting flags. One major goal of our attack strategy was to develop a method to steal flags directly from the victim file system. This can be achieved by successfully exploiting a buffer overflow attack that will trigger a chain of events, beginning with the injection of a piece of shellcode. This shellcode will contain instructions to install a malicious backdoor program (binary format) on their machine and will also run the installed binary. The result would ideally be the stealing of flags directly from the file system and sending it back to our server which would also double as an automatic service to submit the flags received. This component of our strategy which we dub "the backdoor flag monitor" consists of two separate parts, the server and the flag monitoring program, that coordinate with each other achieve the goal of "direct" flag stealing. \\

\noindent The server has three primary functions. It will first and foremost serve the shellcode request for our \texttt{backdoor.sh} script and trigger an install of the program on the victim machine. The second function it has is to listen on a port to be decided and parse HTTP GET URI of the form:\\\\
$<host>:<port>/flag_{1}/flag_{2}/.../flag_{n}$\\\\ where $0 < flag_i \leq n$ depending on how many files are written to by the master server at every three minute interval. Thirdly, after parsing the URI in the GET request, the server submits the list of flags received to the master server. An important thing to note is that these flags submitted may or may not be correct. This is due to an unknown nature of why multiple files in the directory in which the flags are updated are written to or modified. Because of this behavior, we cannot completely rely on the value of the "most updated file" and rather decide to approach the problem by submitting a set of probable flags. The implementation of this approach is defined in our flag monitoring program, which we will dub \texttt{"backdoor.sh"}.\\

\noindent The malicious \texttt{backdoor.sh} works only if assumptions \textbf{(1)} or \textbf{(2)} apply. The program, placed on the victim server, was designed with the knowledge that we've acquired exploring our own server and operates by periodically (every 3 minutes) executing the command \texttt{find /opt/ctf/ -name rw -type d 2>/dev/null}, which will return a list of the most recently updated files in existing \texttt{/rw/} directories that are within \texttt{/opt/ctf/}. The reason why we read a set of files as opposed to simply storing the most recently touched file was due to some unexplained behaviors by the Gamebot/Scriptbot. What had occurred previously was that for an unknown reason, multiple files of a single service would be updated, and because of this behavior, there was no known solution that we had that could pinpoint the correct file to use. Instead we opted to go towards a brute force method described previously. After obtaining the set of the most recently updated files within the past three minutes, the next step consists of the construction of a HTTP GET request which will contain the URI with all the flags appended, with each flag separated by '/'. The packet is then sent through the use of \texttt{curl} and handled by the server.\\

%Still need to add defensive part
\noindent Our initial approach to defensive automation was focused from the network standpoint. We had originally planned to design a tool that would lie between the services and the network, similar to a firewall. We had planned to use this tool to intercept inputs and payloads before forwarding them into the application, monitor traffic and report those that were suspicious. However after several discussions and more research on the game server, it was decided this was infeasible for several reasons. First and foremost had to do with the way the network was set up, namely, NAT was enabled. This made it extremely difficult to differentiate the source and destination of requests and responses. Another obstacle we encountered were potential race conditions that we would need to win every single time. While true that we could theoretically sniff the network, sanitizing inputs would be difficult in this manner as we must not only run a program to sanitize the input, we also have to construct the "cleaned" payload and then forward that to the service before the original payload reaches it. Examples of such difficulties made us switch directions regarding defense, in particular focus on patching the services themselves. \\

% Vishnu -- James -- Please write here about your patch
\noindent Before we could patch any of the files we needed to be have the ability to restore them in case we brought down the service. We also needed required the ability to edit a file and test it before we deployed the patch. To accomplish this, we wrote a \texttt{backup.py} script that could be run from the command line or by other Python scripts. 
This script automatically made two back ups. One named \texttt{<name>\_backup} that was to not be changed and one named \texttt{<name>\_patched} that was meant to be changed. If the option was specified, it also would execute the command \texttt{chown} so the service would not be down when it switched to the patched file.\\\\

\noindent Depending on which language the service was coded in or formatted, we had different approaches to patch them.\\

%c patches
%python patches
%web patches 
%binary patches
\noindent To patch the binary files, we first had to know what was wrong with them. To figure this out we used objdump to get the assembly and .rodata. We first looked at the .rodata to see if there was anything easily identifiable as input for a vulnerability. Then we used a tool that gave us a view of the c files logic (where if statements were). Once we identified a vulnerability, we were able to use the objdump to identify what hex needed to be changed in the elf file. Then we opened the elf file using vim. We used the command \texttt{:\%!xxd} to convert the elf to readable hex, changed the hex we had identified in the objdump, and used \texttt{:\%!xxd -r} to convert it back to an ELF file. We successfully did this on the test server.

\addcontentsline{toc}{section}{Game Day Implementation}
\section*{Game Day Implementation}
The team assumed that the game day server would be similar to the example servers that were set up. This meant that the we could automate those vulnerabilities for the game day and get points very quickly.
The team also knew not everything would be exactly the same, so the scripts that utilized configuration files were created so that a single change could be made and affect all the relating files.\\

\noindent A \texttt{exploit\_and\_submit} script was made to run as a \texttt{cronjob} every minute. This script uses ConfigParser to parse data from the \texttt{config} file provided, which contains information such as the number of services and vulnerabilities. Activated services are placed in a dictionary structure at the top of the script which define which ones to exploit. 
By default, service3 is for web because it needs a different configuration of commands to be executed to be exploited, In the current submission we have, service4 was added with the goal of automating the backdoor deployment by exploiting the service \texttt{backup\_c.c}, which we knew contained a command injection vulnerability. This script replaces keywords like <port>, and <hostname> among others, and automatically opens up a connection with the enemy teams. Exploits are then automatically performed by the script and the acquired flags are trimmed and submitted independently for each active service. The flags and successful/unsuccessful attempts are then logged into 2 files text files with a time stamp appended at the end.


\noindent The entire source code of all the tools used are hosted on Github at \url{https://github.com/theskullcrusher/codeyoloco} in a private repository with Professor Adam Doup\'e added as a collaborator. A Python library with basic setup and requirements files were made available such that additional necessities could be installed into the system as a library whose functions could be imported in an interactive Python (iPython) shell. From there, aggregating the iCTF module's smaller functions into frequently used bigger modules were made possible. 
A \texttt{setup\_environment.sh} script is included to setup powerful shells, virtual environment, git, and other required tools to speed up server configurations by the time of first login. The automation scripts and supporting functions are inside /codeyoloco directory. You will find the \texttt{login}, \texttt{config} and \texttt{exploit\_and\_submit} Python files here. Running \texttt{login.py} after setting the \texttt{config} to the needed values will generate \texttt{root\_ssh.sh} and \texttt{ctf\_ssh.sh} that allow for rapid login to our game server.
Please refer the various \texttt{readme}'s available in the Github for more details on installation and example code snippets on it's use.


\addcontentsline{toc}{section}{Results}
\section*{Results}
We ranked 3rd overall at the end of the CTF, but we for sure think we could have done better. Our automation actually helped us not waste time on flag capturing and submission once the vulnerability was detected but we focused a little bit more on patching the services rather than everyone focusing on finding and exploiting more. This slowed us down, along with the issues that we faced in patching the services that put them down, resulting in additional loss points. Our key weapon was the backdoor service and we successfully broke into a couple of machines to install the backdoor there too. One thing we failed to do is directory traversal across all services to get flags, which could have helped us get 1000 points every round for the unexploited c service. But, unfortunately we faced some issues with flags being pushed out of the vulnerable system, which was not anticipated and that resulted in this attempt not succeeding. Even then, we are pretty proud of pursuing this approach which would have been a one-for-all solution for the CTF Game. Our automation tool working out with minimal effort ensured us the 3rd position even after we had a lot of downtime on our own services.

\addcontentsline{toc}{section}{Future Improvements}
\section*{Future Improvements}
We made a pretty good overall effort considering our approach, the difficulties we faced and the assumptions we had to make. Having a better idea now, we do have a few improvements that could be worked upon:
\begin{enumerate}
	\item Establishing a better method of communication between teammates would have made dividing up teamwork more efficient. Communication between members of the team oftentimes was chaotic and unorganized in our competitive atmosphere, especially when our services began going down. Instead of frantically calling teammates over, next time we know to have a more robust monitoring system (perhaps automated logs) that will aid in notifying us when our services are down, and to immediately patch them without having to call out to the patching team.
	\item After verifying that our proof of concept backdoor flag stealer was indeed viable, plenty more could be added to improve it. One of the most important things we could have improved on was making the tool more configurable. One of the earlier troubles we had installing the backdoor on other machines was the fact that we needed to manually reconfigure many of the variables such as file path and IP addresses among others. If we could dynamically change those settings in real time, the deployment time could have been halved.
	\item Rather than giving up on our previous "sniffer" idea, a separate team could have continued working on it as a secondary project. Having the ability to monitor our own network have been extremely beneficial for defensive reasons. By being able to check incoming and outgoing packets and inspecting their payloads, our defense team would have an easier time patching services as the insights of knowing what kind of vulnerabilities other teams are trying to exploit give us clues as to where the vulnerabilities are in the services attacked.
\end{enumerate}	
	
\addcontentsline{toc}{section}{Conclusion}
\section*{Conclusion}	
Ultimately, this CTF was both an enjoyable and intellectually stimulating exercise. When choosing a direction for our project, we explored many different possibilities before finally settling on our final design. These ideas included a local reflector to filter malicious traffic, a network sniffer on opposing team servers, and other similar variations. Trying all these gave us a good feel for which types of attacks are feasible and which are not, and these lessons inspired our final design - an automated exploit tool coupled with a remote backdoor.\\
Trying to make our project work in the real competition proved to be very exciting. Of course, the things did not work quite as well as we had hoped, but we learned a lot from it. The file system was structured differently than on the practice server, which initially made the backdoor fail. Moreover, the patching team accidentally shut down our services for a short period, resulting in the loss of points. Overall, though, the project was a success. The automated submission tool worked nearly to perfection, we were able to patch two of our services fairly quickly, and the backdoor was able to be run in the end (though at that point most opposing team servers had been patched). Of these, the most satisfying success was that the backdoor service did indeed prove to be a viable attack. This was always a question in our mind, as we were sort of coding blind, without a solid idea of how things would look on the real server.\\

\noindent Most important to us personally, we managed to organize a group of seven people in a very high-pressure environment to work together and place 3rd overall. As this was the first time most of us had ever participated in a CTF, this was a very proud moment, and we are quite satisfied with the achievement. We are confident that the knowledge and experience gained through this project will be valuable as many of us go on to pursue careers in security.

\end{document}

%-------------------------------------------------------------------------------
% SNIPPETS
%-------------------------------------------------------------------------------

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=0.8\textwidth]{file_name}
%	\caption{}
%	\centering
%	\label{label:file_name}
%\end{figure}

%\begin{figure}[!ht]
%	\centering
%	\includegraphics[width=0.8\textwidth]{graph}
%	\caption{Blood pressure ranges and associated level of hypertension (American Heart Association, 2013).}
%	\centering
%	\label{label:graph}
%\end{figure}

%\begin{wrapfigure}{r}{0.30\textwidth}
%	\vspace{-40pt}
%	\begin{center}
%		\includegraphics[width=0.29\textwidth]{file_name}
%	\end{center}
%	\vspace{-20pt}
%	\caption{}
%	\label{label:file_name}
%\end{wrapfigure}

%\begin{wrapfigure}{r}{0.45\textwidth}
%	\begin{center}
%		\includegraphics[width=0.29\textwidth]{manometer}
%	\end{center}
%	\caption{Aneroid sphygmomanometer with stethoscope (Medicalexpo, 2012).}
%	\label{label:manometer}
%\end{wrapfigure}

%\begin{table}[!ht]\footnotesize
%	\centering
%	\begin{tabular}{cccccc}
%	\toprule
%	\multicolumn{2}{c} {Pearson's correlation test} & \multicolumn{4}{c} {Independent t-test} \\
%	\midrule	
%	\multicolumn{2}{c} {Gender} & \multicolumn{2}{c} {Activity level} & \multicolumn{2}{c} {Gender} \\
%	\midrule
%	Males & Females & 1st level & 6th level & Males & Females \\
%	\midrule
%	\multicolumn{2}{c} {BMI vs. SP} & \multicolumn{2}{c} {Systolic pressure} & \multicolumn{2}{c} {Systolic Pressure} \\
%	\multicolumn{2}{c} {BMI vs. DP} & \multicolumn{2}{c} {Diastolic pressure} & \multicolumn{2}{c} {Diastolic pressure} \\
%	\multicolumn{2}{c} {BMI vs. MAP} & \multicolumn{2}{c} {MAP} & \multicolumn{2}{c} {MAP} \\
%	\multicolumn{2}{c} {W:H ratio vs. SP} & \multicolumn{2}{c} {BMI} & \multicolumn{2}{c} {BMI} \\
%	\multicolumn{2}{c} {W:H ratio vs. DP} & \multicolumn{2}{c} {W:H ratio} & \multicolumn{2}{c} {W:H ratio} \\
%	\multicolumn{2}{c} {W:H ratio vs. MAP} & \multicolumn{2}{c} {\% Body fat} & \multicolumn{2}{c} {\% Body fat} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Height} & \multicolumn{2}{c} {Height} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Weight} & \multicolumn{2}{c} {Weight} \\
%	\multicolumn{2}{c} {} & \multicolumn{2}{c} {Heart rate} & \multicolumn{2}{c} {Heart rate} \\
%	\bottomrule
%	\end{tabular}
%	\caption{Parameters that were analysed and related statistical test performed for current study. BMI - body mass index; SP - systolic pressure; DP - diastolic pressure; MAP - mean arterial pressure; W:H ratio - waist to hip ratio.}
%	\label{label:tests}
%\end{table}